{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание. Логистическая регрессия\n",
        "\n",
        "В этом домашнем задании мы изучим логистическую регрессию.\n",
        "# Часть 1. Реализация логистической регрессии с регуляризацией\n",
        "Вам предстоит применить $L_2$-регуляризацию к алгоритму логистической регрессии. На семинарском занятии мы применяли $L_2$-регуляризацию к линейной регрессии, а также реализовали логистическую регрессию. Поэтому все необходимые компоненты у нас уже есть, осталось только их соединить.\n"
      ],
      "metadata": {
        "id": "Fk-Ooj0C0brz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "XHEJ3izio8uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1. Реализуйте алгоритм логрегрессии с $L_2$-регуляризацией.\n",
        "\n",
        "Вам необходимо реализовать лишь методы `basic_term`, `regularization_term` и `grad`: остальное мы сделали за вас.\n",
        "\n",
        "* Метод `basic_term(self, X, y, logits)` вычисляет основное слагаемое градиента логистической функции потерь по формуле с семинара:\n",
        "\n",
        "$$\\nabla_w L = -\\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}y^i\\cdot\\left(1 - \\frac{1}{1 + \\mathrm{exp}(-y^i \\cdot \\langle w, x^i \\rangle)}\\right) \\cdot x^i = -\\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}y^i\\cdot\\left(1 - \\sigma(y^i \\cdot \\langle w, x^i \\rangle)\\right) \\cdot x^i.$$\n",
        "\n",
        "* Метод `regularization_term(self, weights)` вычисляет градиент регуляризационного слагаемого функции потерь. Само регуляризационное слагаемое имеет вид\n",
        "$$L_2 (w) = \\sum_{j=1}^{n}w_j^2.$$\n",
        "Производная регуляризационного слагаемого по каждому весу вычисляется по формуле\n",
        "$$\\frac{\\partial L_2}{\\partial w_1} = 2 \\cdot w_1.$$\n",
        "Таким образом, градиент по вектору весов равен\n",
        "$$\\nabla_{w}L_2 = (0, 2w_1, \\ldots, 2w_k)$$\n",
        "(не учитывая $w_0$).\n",
        "\n",
        "* метод `grad(self, X, y, logits, weights)` складывает слагаемые с учетом константы регуляризации $C$: $$\\nabla_{w}L + C\\cdot \\nabla_{w}L_2.$$"
      ],
      "metadata": {
        "id": "Thh5YTfB3XWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyL2LogisticRegression(object):\n",
        "    def __init__(self, C=1):\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.C = C\n",
        "\n",
        "    def sigmoid(self, t):\n",
        "        return 1. / (1 + np.exp(-t))\n",
        "\n",
        "    def basic_term(self, X, y, logits):\n",
        "        # Вычисляет градиент логистической функции потерь по весам алгоритма\n",
        "        # (исключая регуляризационное слагаемое)\n",
        "        # ВАШ КОД\n",
        "        grad = #ВАШ КОД. Вычислите слагаемое от логистической функции потерь\n",
        "        return grad\n",
        "\n",
        "    def regularization_term(self, weights):\n",
        "        # Вычисляет регуляризационное слагаемое градиента функции потерь\n",
        "        # (без домножения на константу регуляриации)\n",
        "        grad = #ВАШ КОД\n",
        "        return grad\n",
        "\n",
        "    def grad(self, X, y, logits, weights):\n",
        "        # Принимает на вход X, y, logits и вычисляет градиент логистической\n",
        "        # функции потерь (включая регуляризационное слагаемое).\n",
        "        # ВАШ КОД. Вычислите basic_term и regularization_term.\n",
        "        grad = #ВАШ КОД. Сложите две компоненты (не забудьте домножить на C)\n",
        "        return grad\n",
        "\n",
        "    def fit(self, X, y, max_iter=1000, lr=0.1):\n",
        "        # Принимает на вход X, y и вычисляет веса по данной выборке.\n",
        "        # Множество допустимых классов: {1, -1}\n",
        "        # Не забудьте про фиктивный признак, равный 1!\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        y = 2 * y - 1\n",
        "\n",
        "        # Добавляем признак из единиц\n",
        "        X = np.hstack([np.ones([X.shape[0], 1]), X])  # [ell, n]\n",
        "\n",
        "        l, n = X.shape\n",
        "        # Инициализируем веса\n",
        "        weights = np.random.randn(n)\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        for iter_num in range(max_iter):\n",
        "            # calculate grad\n",
        "            logits = (X @ weights.reshape(n, 1)).ravel()  # [ell]\n",
        "            grad = self.grad(X, y, logits, weights)\n",
        "            # update weights\n",
        "            weights -= grad * lr\n",
        "\n",
        "            # calculate loss\n",
        "            loss = np.mean(np.log(1 + np.exp(-y * logits))) + self.C * np.sum(weights[1:] ** 2)\n",
        "            losses.append(loss)\n",
        "\n",
        "        # assign coef, intersept\n",
        "        self.coef_ = weights[1:]\n",
        "        self.intercept_ = weights[0]\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Принимает на вход X и возвращает ответы модели\n",
        "        X = np.array(X)\n",
        "        X = np.hstack([np.ones([X.shape[0], 1]), X])  # [ell, n]\n",
        "        weights = np.concatenate([self.intercept_.reshape([1]), self.coef_])\n",
        "        logits = (X @ weights.reshape(-1, 1))  # [ell, 1]\n",
        "\n",
        "        return self.sigmoid(logits)\n",
        "\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return self.predict_proba(X) >= threshold"
      ],
      "metadata": {
        "id": "wGwrypvm3Q2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Протестируем наш класс\n",
        "Для этого проверим, что вычисление градиентов правильно работает на каких-то случайных входах."
      ],
      "metadata": {
        "id": "VURHV1bs-nNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_clf = MyL2LogisticRegression(C=10.)\n",
        "X = np.arange(6).reshape(2, 3)\n",
        "y = np.array([0, 1])\n",
        "weights = np.array([-1., 1., 2])\n",
        "logits = X @ weights\n",
        "\n",
        "b_t = dummy_clf.basic_term(X, y, logits)\n",
        "reg_t = dummy_clf.regularization_term(weights)\n",
        "grad = dummy_clf.grad(X, y, logits, weights)\n",
        "\n",
        "b_t, reg_t, grad"
      ],
      "metadata": {
        "id": "c34254bg9dnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.allclose(b_t, [-2.50521328e-05, -3.34028437e-05, -4.17535546e-05])\n",
        "assert np.allclose(reg_t, [0., 2., 4.])\n",
        "assert np.allclose(grad, [-2.50521328e-05,  1.99999666e+01,  3.99999582e+01])"
      ],
      "metadata": {
        "id": "2T6WPSLT_0l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если тесты прошли, можно смело сдавать реализацию на Stepik!"
      ],
      "metadata": {
        "id": "lzZW4aU-UaPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Проверим на датасете blobs с семинара"
      ],
      "metadata": {
        "id": "UV5CEcr_A18w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=1000, centers=[[-10, 15],[2,-0.5]], cluster_std=9, random_state=42)\n",
        "# Здесь мы взяли другие центры, чтобы регуляризация имела значимый эффект\n",
        "\n",
        "\n",
        "colors = (\"red\", \"green\")\n",
        "colored_y = np.zeros(y.size, dtype=str)\n",
        "\n",
        "for i, cl in enumerate([0,1]):\n",
        "    colored_y[y == cl] = str(colors[i])\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colored_y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-pL1WZ88Asog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2. Применение созданного класса к искусственным данным\n",
        "Создайте объект класса `MyL2LogisticRegression`, рассмотрев параметр регуляризации $С = 0.01$.\n",
        "\n",
        "Обучите модель с помощью метода `fit` с параметром `max_iter=1000`. Какие у модели получились веса? Введите полученные веса на Stepik.\n",
        "\n",
        "**ВАЖНО!!! Если параметр `max_iter` в методе `fit` выбрать другим, то ответ получится другой, и Stepik его не зачтет!**  "
      ],
      "metadata": {
        "id": "h_I1xfnzIPsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# При C=0.01\n",
        "\n",
        "clf = # ВАШ КОД. Создайте модель\n",
        "\n",
        "losses = # ВАШ КОД. Обучите модель\n",
        "\n",
        "coef, intercept = clf.coef_, clf.intercept_\n",
        "\n",
        "coef, intercept"
      ],
      "metadata": {
        "id": "jG6nvcqOBAoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим, что лосс падает."
      ],
      "metadata": {
        "id": "-1dptUwEJnbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(losses, label='loss')\n",
        "plt.legend(fontsize=14)\n",
        "plt.xlabel('iter', fontsize=14)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "EPaOjKY9BLB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3. Какое accuracy дает наша модель на обучающих данных?\n",
        "Предскажите значения целевой переменной, используя метод predict. Вычислите accuracy. Сдайте полученное значение на Stepik.\n"
      ],
      "metadata": {
        "id": "W5HEHCWgKMhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predictions = #ВАШ КОД\n",
        "score = #ВАШ КОД\n",
        "print(f'Model accuracy = {score}')"
      ],
      "metadata": {
        "id": "MNXbtF5UKTOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визуализируем работу модели."
      ],
      "metadata": {
        "id": "jzzjY0n4J_gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "eps = 0.1\n",
        "xx, yy = np.meshgrid(np.linspace(np.min(X[:,0]) - eps, np.max(X[:,0]) + eps, 500),\n",
        "                     np.linspace(np.min(X[:,1]) - eps, np.max(X[:,1]) + eps, 500))\n",
        "\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
        "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colored_y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8hur1HFpBRcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4. Качество в зависимости от константы регуляризации\n",
        "Проделайте процедуру из заданий 2, 3 для константы регуляризации $C=2$ и вычислите accuracy, а также проанализируйте, как отличаются веса моделей и разделяющие поверхности. Постройте графики сходимости функции потерь и попробуйте объяснить полученный эффект. Выберите верные утверждения в задании на Stepik.\n"
      ],
      "metadata": {
        "id": "qunOAeMhKDIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ВАШ КОД"
      ],
      "metadata": {
        "id": "aFAL2shKEu1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**БОНУС**. Что будет, если взять константу регуляризации слишком большой, например, $C=100$? Попробуйте обучить такую модель. Объясните полученный эффект. Подумайте, как это можно было бы исправить."
      ],
      "metadata": {
        "id": "MgvLK2tSM7yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ВАШ КОД"
      ],
      "metadata": {
        "id": "W2UkaSB1M-i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 2. Использование логистической регрессии из sklearn\n",
        "Мы с вами самостоятельно реализовали обучение логистической регрессии. Теперь давайте поработаем с коробочной реализацией логистической регрессии из библиотеки sklearn. Делать мы это будем на том же наборе данных `forest`, с которым мы уже работали в домашнем задании \"[Пайплайн машинного обучения](https://colab.research.google.com/drive/1MXMA7Zp2R0RiJ5fUlM9hr2Ww5LBrI4db?usp=sharing#scrollTo=YD0NXyUYKxY7)\".\n",
        "\n",
        "Скачайте набор данных по [ссылке](https://drive.google.com/file/d/1T9jXx2qBUMw03BsB2IdZpThDDfKKmrSK/view?usp=sharing). Загрузим и разобьем данные на обучающее и тестовое множества, как и в предыдущем домашнем задании."
      ],
      "metadata": {
        "id": "hpzo6FSRMdoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1T9jXx2qBUMw03BsB2IdZpThDDfKKmrSK\n",
        "\n",
        "all_data = pd.read_csv('forest_dataset.csv')\n",
        "all_data.head()"
      ],
      "metadata": {
        "id": "TgsYiydeGB9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "-OjKIsFdOFGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = all_data[all_data.columns[-1]].values\n",
        "feature_matrix = all_data[all_data.columns[:-1]].values\n",
        "\n",
        "train_feature_matrix, test_feature_matrix, train_labels, test_labels = train_test_split(\n",
        "    feature_matrix, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vI9BaBs1N_YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В модели `sklearn.linear_model.LogisticRegression` константа регуляризации $C$ работает иначе, чем у нас:\n",
        "* Малые значения $C$ соответствуют сильной регуляризации;\n",
        "* Большие значения $C$ соответствуют слабой регуляризации.\n",
        "\n",
        "## Задание 5. Исследование регуляризации в sklearn\n",
        "\n",
        "Для нашего набора данных обучите модель `sklearn.linear_model.LogisticRegression` на `(train_feature_matrix, train_labels)`, перебирая параметр $C$ по сетке, указанной ниже. Можете выполнить перебор вручную или использовать методы `sklearn`."
      ],
      "metadata": {
        "id": "hPJ_vh-UOaZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C_grid = np.logspace(-5, 5, 11)\n",
        "C_grid"
      ],
      "metadata": {
        "id": "o9wQ50T9P1Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для каждого значения $C$ вычислите `accuracy_score` на обучающем и тестовом множествах. Запишите результаты в массив `train_accuracies` и `test_accuracies`."
      ],
      "metadata": {
        "id": "m_vjRgW2SgDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "U0rHlxA_OG9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for C in C_grid:\n",
        "    # ВАШ КОД. Заполните массивы train_accuracies и test_accuracies.\n",
        "    # ИСПОЛЬЗУЙТЕ LogisticRegression из sklearn, а не вашу имплементацию!"
      ],
      "metadata": {
        "id": "0H81h2W1QmEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим график зависимости accuracy от $C$. Сравните кривые качества на обучающем и тестовом множествах. Для какого значения $C$ получилось наилучшее качество на **тестовом множестве**? Ответ сдайте на Stepik."
      ],
      "metadata": {
        "id": "n8POgPYKSSG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(C_grid, train_accuracies, label='train accuracy')\n",
        "plt.plot(C_grid, test_accuracies, label='test accuracy')\n",
        "plt.xlabel('C')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xscale('log')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sunaEtBFRhu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}